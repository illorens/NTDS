{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 4: graph signal processing\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Rodrigo Pena](https://people.epfl.ch/254838), [EPFL LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `01`\n",
    "* Students: `Yassine Zouaghi, Timothée Bornet dit Vorgeat, Pol Boudou, Icíar Lloréns Jover`\n",
    "* Dataset: `IMDb Films and Crew`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this milestone is to do some Graph Signal Processing (GSP) on the data of your project.\n",
    "\n",
    "### A note about plotting\n",
    "\n",
    "There are several questions in this milestone that ask you to plot a signal on your network.\n",
    "There are several ways from which you could approach it.\n",
    "In all cases, compute the position of the nodes a single time at the beginning, as this is likely to be a costly operation.\n",
    "Using a single layout for all the graph plots will also make it easier to compare the plots.\n",
    "Indeed, the only thing changing between plots is the signal displayed.\n",
    "You can represent the features/labels lying on the graph via node **colors**.\n",
    "To do so, make sure to have a consistent color map throughout and remember to display a colorbar and scale in all plots, so that we can tell what numbers the colors represent.\n",
    "\n",
    "* An option is to use the **Laplacian eigenmaps** that you have seen in the previous milestone to embed your graph on the plane. For example:\n",
    "  ```\n",
    "  from matplotlib import pyplot as plt\n",
    "  plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=signal, alpha=0.5)\n",
    "  plt.colorbar()\n",
    "  ```\n",
    "* Another option is to use the plotting capabilities of **[NetworkX](https://networkx.github.io)**.\n",
    "  See the documentation of its [drawing methods](https://networkx.github.io/documentation/stable/reference/drawing.html).\n",
    "  For example:\n",
    "  ```\n",
    "  import networkx as nx\n",
    "  graph = nx.from_scipy_sparse_matrix(adjacency)\n",
    "  coords = nx.spring_layout(graph)  # Force-directed layout.\n",
    "  coords = eigenvectors[:, 1:3]  # Laplacian eigenmaps.\n",
    "  nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=signal)\n",
    "  nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "  ```\n",
    "* Another option is to use the plotting capabilities of the **[PyGSP](https://github.com/epfl-lts2/pygsp)**, a Python package for Graph Signal Processing.\n",
    "  **Note that your are forbidden to use the PyGSP for anything else than plotting.**\n",
    "  See the documentation of its [plotting utilities](https://pygsp.readthedocs.io/en/stable/reference/plotting.html).\n",
    "  For example:\n",
    "  ```\n",
    "  import pygsp as pg\n",
    "  graph = pg.graphs.Graph(adjacency)\n",
    "  graph.set_coordinates('spring')  # Force-directed layout.\n",
    "  graph.set_coordinates(eigenvectors[:, 1:3])  # Laplacian eigenmaps.\n",
    "  graph.plot_signal(signal)\n",
    "  ```\n",
    "* Yet another option is to save your graph on disk, use **[Gephi](https://gephi.org)** externally, to visualize the graph, save the graph with the Gephi coordinates and finally load the nodes coordinates back into the notebook.\n",
    "\n",
    "We encourage you to try all the above methods before making your choice. Then be consistent and use only one throughout the milestone.\n",
    "NetworkX and PyGSP should already be installed in your environement. If that's not the case, install with `conda install networkx pygsp` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `No module named 'pyunlocbox'` error when running the below cell, install the [pyunlocbox](https://github.com/epfl-lts2/pyunlocbox) with `conda install pyunlocbox` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csgraph\n",
    "import scipy.sparse.linalg\n",
    "from matplotlib import pyplot as plt\n",
    "from pyunlocbox import functions, solvers\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this milestone, all we will need is a set of features/labels for each of the nodes on the network, as well as the Laplacian, $L,$ and Gradient, $\\nabla_G,$ matrices that you have computed for your network while working on milestone 3.\n",
    "\n",
    "Import those objects in the cell below (or recompute the Laplacian and Gradient from your stored adjacency matrix, if you wish).\n",
    "\n",
    "_Note_: If your features/labels are not floating-point numbers, please convert them. For example, if your data has labels \"cat\" and \"dog\" for nodes that represent cats or dogs, respectively, you may assign the number `1.0` for the label \"cat\" and the number `-1.0` for the label \"dog\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"DataMS1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the adjacency matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_adjacency = np.load(DATA_PATH + 'biggest_adjacency.npy')\n",
    "\n",
    "degree_weighted_imdb = biggest_adjacency.sum(1)\n",
    "D = np.diag(degree_weighted_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Extract the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_components(adjacency):\n",
    "    \"\"\"Find the connected components of a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of numpy arrays\n",
    "        A list of adjacency matrices, one per connected component.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert adjacency matrix into an adjacency list\n",
    "    # This makes it easier to navigate through the nodes\n",
    "    adj_list = []\n",
    "    for i in range(0, len(adjacency)):\n",
    "        adj_list.append(list(np.nonzero(adjacency[i])[0]))\n",
    "        \n",
    "    # Each node will be assigned a label (starting from 1) according \n",
    "    # to which connected component it belongs. If the label is 0, \n",
    "    # it means it has not been visited yet.\n",
    "    nodes = np.zeros(len(adjacency))\n",
    "    # n counts the number of connected components\n",
    "    n = 0\n",
    "    \n",
    "    while (np.count_nonzero(nodes) != len(adjacency)):\n",
    "        n = n+1\n",
    "        # We start from the first node that has not been visited\n",
    "        start = np.where(nodes == 0)[0][0]\n",
    "        queue = []\n",
    "        visited = set()\n",
    "\n",
    "        queue.append(start)\n",
    "        visited.add(start)\n",
    "\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            for node in adj_list[current]:\n",
    "                if node not in visited:\n",
    "                    queue.append(node)\n",
    "                    visited.add(node)\n",
    "        \n",
    "        visited = list(visited)\n",
    "        # Visited nodes are assigned their label\n",
    "        nodes[visited] = n\n",
    "        \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the labels from our features. The labels are the genres. However, since our graph is not connected and we are only taking the nodes from the biggest connected component, we need to find out which nodes belong to that component so as to take only the labels assigned to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action, Adventure, Fantasy, Science Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure, Fantasy, Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Action, Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action, Adventure, Science Fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        genres\n",
       "0  Action, Adventure, Fantasy, Science Fiction\n",
       "1                   Adventure, Fantasy, Action\n",
       "2                     Action, Adventure, Crime\n",
       "3               Action, Crime, Drama, Thriller\n",
       "4           Action, Adventure, Science Fiction"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency =  np.load(DATA_PATH + 'adjacency.npy')\n",
    "features = pd.read_csv(DATA_PATH + 'features.csv').drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "\n",
    "# Find the nodes from the biggest connected component and select the features assigned to those nodes\n",
    "nodes = find_components(adjacency)\n",
    "idx = [i for i in range(len(nodes)) if nodes[i] == 1]\n",
    "features = features.iloc[idx]\n",
    "\n",
    "# Each node's labels is initially the list of genres associated to it. When the genre is empty for a \n",
    "# node, we set it to Other.\n",
    "genres = features.genres\n",
    "labels = genres.apply(lambda x: list(pd.read_json(x, dtype=False).name) if x != '[]' else ['Other'])\n",
    "labels = labels.apply(lambda x: \", \".join(x))\n",
    "\n",
    "# Finally, we create a dataframe \n",
    "labels_df = pd.DataFrame(labels.reset_index(drop=True))\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe with each node index containing a list of genres, we use a one hot encoding to transform the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def getCatFeatures(df, Col):\n",
    "    vectorizer = CountVectorizer(binary=True,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 tokenizer=lambda x:[a.strip() for a in x.split(',')])\n",
    "    return (vectorizer.fit_transform(df[Col]), vectorizer.vocabulary_)\n",
    "Sparse_labels, vocab_labels = getCatFeatures(labels_df,'genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a binary label signal. We achieve this by taking only one genre. This way, if a movie belongs to the genre, its label is 1, otherwise its label is 0. We decided to use 'Action' as a determining genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_oh = pd.SparseDataFrame([pd.SparseSeries(Sparse_labels[i].toarray().ravel()) for i in np.arange(Sparse_labels.shape[0])],\n",
    "                               columns = sorted(list(vocab_labels)))\n",
    "label_signal = np.array(labels_oh.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Number of nodes and laplacian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes =  len(biggest_adjacency)\n",
    "laplacian = sparse.csgraph.laplacian(biggest_adjacency, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Gradient**\n",
    "\n",
    "We have used a normalized Laplacian. In order to compute the normalized Gradient matrix, we cannot use the incidence matrix anymore because our graph is weighted. Given the number of nodes $N$ and the number of edges $E$, \n",
    "\n",
    "- for each edge the gradient is $\\nabla f [i,j] = \\sqrt{w_{ij}}(\\frac{f(j)}{\\sqrt{d_j}}-\\frac{f(i)}{\\sqrt{d_i}})$.\n",
    "- the gradient matrix is of size $E\\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bf1922b95fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_nodes\u001b[0m  \u001b[0;31m# number of nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiggest_adjacency\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of edges (non-zero entries of A)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# edge index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = n_nodes  # number of nodes\n",
    "E = np.sum(biggest_adjacency > 0)  # number of edges (non-zero entries of A)\n",
    "gradient = np.zeros((E, N))\n",
    "eij = 0  # edge index\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        wij = biggest_adjacency[i, j]\n",
    "        if wij > 0:\n",
    "            gradient[eij, i] = - np.sqrt(wij/D[i,i])\n",
    "            gradient[eij, j] = np.sqrt(wij/D[j,j])\n",
    "            eij = eij + 1  # increment the edge index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of edges: {}, number of nodes: {} \\nGradient shape: {}\".format(E, N, gradient.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert our gradient to a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = sparse.csr_matrix(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Graph Fourier Transform\n",
    "\n",
    "In this section we will observe how your feature/label vector looks like in the \"Graph Fourier\" domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Compute the Fourier basis vectors and the Laplacian eigenvalues. Make sure to order those from smaller to larger, $\\lambda_0 \\leq \\lambda_1 \\leq \\dots \\leq \\lambda_{N-1},$ and use the same ordering for the Fourier basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# We compute ordered eigenvalues e and graph Fourier basis U with eigh\n",
    "e , U = scipy.linalg.eigh(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first 3 and the last Fourier basis vectors as signals on your graph. Clearly indicate which plot belongs to which basis vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Graph creation\n",
    "graph = nx.from_scipy_sparse_matrix(sparse.csr_matrix(biggest_adjacency))\n",
    "coords = U[:,1:3]  # Laplacian eigenmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=U[:,0])\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"First Fourier basis vector (null) as a signal on our graph\", y=1.08)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=U[:,1])\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Second Fourier basis vector as a signal on our graph\", y=1.08)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=U[:,2])\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Third Fourier basis vector as a signal on our graph\", y=1.08)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=U[:,-1])\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Last Fourier basis vector as a signal on our graph\", y=1.08)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What can you observe in terms of local variations when comparing the basis vectors corresponding to the smallest eigenvalues to those corresponding to the largest eigenvalue? How would this justify the interpretation of the eigenvalues as \"graph frequencies\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "***As our graph (biggest component only) is composed of a very tight cluster and a few outliers, the information presented by an individual graph alone does not offer much information about the nodes in the cluster (high-degree nodes).***\n",
    "\n",
    "**The first Fourier basis vector (associated to eigenvalue 0) presents small but observable variations (a very tight color range). However, the second and third eigenvectors present significantly bigger variations (the range is around 20 times larger). Finally, the last Fourier basis vector presents the biggest variations (from $-0.6$ to $0.6$).** \n",
    "\n",
    "**In fact, we can say that the eigenvalues for which the corresponding eigenvectors have small variations are labeled as low frequencies and vice-versa.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Implement a function that returns the Graph Fourier Transform (GFT) of a given vector $x \\in \\mathbb{R}^{N},$ with respect to your graph, and a function that computes the corresponding inverse GFT (iGFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def GFT(x):\n",
    "    return np.matmul(U.T, x);\n",
    "\n",
    "def iGFT(x):\n",
    "    return np.matmul(U, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Plot your feature/label vector as a signal on your graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=label_signal)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"First Fourier basis vector as a signal on our graph\", y=1.08)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As our label signal is 1 if the movie is labeled \"Action\" and 0 else, most of the nodes in our graph are labeled \"0\" (represented in dark blue)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the absolute values of the GFT of your feature/label signal as a function of the graph eigenvalues. Make sure to add a marker indicating the position of each graph eigenvalue, and remember to properly name the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "absolute_GFT_labels = np.abs(GFT(label_signal))\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.stem(absolute_GFT_labels)\n",
    "plt.title(\"Absolute GFT of labels' vector\")\n",
    "plt.xlabel(\"Eigenvalues\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Discuss the behavior of the GFT that you plotted in the last question via comparing the plot of your label signal and those of the Fourier basis of Question 1. Would you consider your labels a \"low-pass\" or \"high-pass\" signal, or yet something else entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the GFT of our labels vector in the spectral domain, we clearly see that our label signal acts as a low pass filter (since it has high values for low frequencies and low values fow high frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## 2 - Filtering on graphs\n",
    "\n",
    "In this section we will check how filtered Dirac impulses diffuse on your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "### Question 6 \n",
    "\n",
    "Implement the following three filter kernels and the graph filtering operation.\n",
    "\n",
    "- The **heat kernel** is supposed to take in a vector of eigenvalues `e` and a parameter `t` and output a vector of evaluations of the heat kernel at those eigenvalues (see the course slides for help).\n",
    "- The **inverse filter** kernel is supposed to take in a vector of eigenvalues `e` and a parameter `t` and implement spectrally the  filter defined in the node domain by $f_{out}  = (I + t L)^{-1} f_{in},$ where $f_{in}, f_{out} \\in \\mathbb{R}^{N}$ are, repectively, the input and output signals to the filter.\n",
    "- The **rectangle kernel** takes in a vector of eigenvalues `e` and parameters `l_min` and `l_max` and returns `1.0` at coordinates satisfying $(e[l] \\geq l_{min}) \\wedge (e[l] \\leq l_{max}),$ and `0.0` otherwise.\n",
    "- The **graph filtering** operation takes a graph signal $x \\in \\mathbb{R}^{N}$, a spectral graph `kernel` and a set of keyworded variables, and returns the corresponding filtered signal.\n",
    "    - _Hint:_ Remember that you have implemented the `GFT` and `iGFT` operations in Question 3.\n",
    "    - The `**kwargs` is a placeholder to collect supplementary pairs of keyword-values that are not known by the implementation before execution time.\n",
    "      The `kwargs` variable is a dictionary whose keyes and values are the parameter names and values.\n",
    "      This is useful to allow both `graph_filter(x, heat_kernel, tau=1.0)` and `graph_filter(x, rectangle_kernel, lambda_min=0.0, lambda_max=1.0)` to be valid calls from the same implementation.\n",
    "      One can then defer the keyword-value assignment to the `kernel` call: `foo = kernel(bar, **kwargs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def heat_kernel(eig, tau):\n",
    "    return np.exp(eig * (tau))\n",
    "\n",
    "def inverse_kernel(eig, tau):  \n",
    "    return 1./(np.ones(eig.shape) + tau * eig)\n",
    "\n",
    "def rectangle_kernel(eig, l_min, l_max):\n",
    "    rect = eig.copy()\n",
    "    rect[np.where(rect > l_max)] = 0\n",
    "    rect[np.where(rect < l_min)] = 0\n",
    "    rect[rect > 0] = 1\n",
    "    return rect\n",
    "\n",
    "def graph_filter(x, kernel, **kwargs):\n",
    "    GFT_x = GFT(x)\n",
    "    filtered_x = np.matmul(np.diag(kernel(e, **kwargs)),GFT_x)\n",
    "    return iGFT(filtered_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Plot all three filter kernels in the spectral domain. Remember to properly name the axes and title the plots. Choose filter parameters that best approximate the behavior of the GFT of your feature/label signal (as seen in Question 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ([original, ax1], [ax2, ax3]) = plt.subplots(2, 2, figsize=(20, 10))\n",
    "f.suptitle(\"Kernels in the spectral domain of our graph\", y=1.08, fontweight=\"bold\")\n",
    "\n",
    "original.stem(absolute_GFT_labels)\n",
    "original.set_title(\"Absolute GFT of labels' vector\")\n",
    "original.set_xlabel(\"Eigenvalues\")\n",
    "original.set_ylabel(\"Magnitude\")\n",
    "\n",
    "ax1.stem(heat_kernel(e, -1))\n",
    "ax1.set_title(\"Heat kernel in the spectral domain\")\n",
    "ax1.set_xlabel(\"Eigenvalues\")\n",
    "ax1.set_ylabel(\"Kernel\")\n",
    "\n",
    "ax2.stem(inverse_kernel(e, 1.5))\n",
    "ax2.set_title(\"Inverse kernel in the spectral domain\")\n",
    "ax2.set_xlabel(\"Eigenvalues\")\n",
    "ax2.set_ylabel(\"Kernel\")\n",
    "\n",
    "ax3.stem(rectangle_kernel(e, 0,0.1))\n",
    "ax3.set_title(\"Rectangle kernel in the spectral domain\")\n",
    "ax3.set_xlabel(\"Eigenvalues\")\n",
    "ax3.set_ylabel(\"Kernel\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Consider two Dirac impulses arbitrarily placed on your graph. Plot their filtered versions by the three filter kernels implemented in Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As we already know, our graph is composed of very few low-degree nodes. This makes our graph visualisations not very significant. For this question, we have decided to place several Diracs (not just 2) specifically on the low-degree nodes of our clustered graph.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find outliers nodes in our graph\n",
    "degrees = np.asarray([val for (node, val) in graph.degree()])\n",
    "np.where( degrees == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define 38 diracs on one-degree nodes\n",
    "dirac = np.zeros(n_nodes)\n",
    "dirac[np.where( degrees == 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_filtered_dirac = graph_filter(dirac, heat_kernel, tau=-1)\n",
    "inverse_filtered_dirac = graph_filter(dirac, inverse_kernel, tau=1.5)\n",
    "rectangle_filtered_dirac = graph_filter(dirac, rectangle_kernel, l_min=0, l_max=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=heat_filtered_dirac)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Heat kernel Dirac filtration as a signal on our graph\", y=1.02, fontsize=15)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=inverse_filtered_dirac)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Inverse kernel Dirac filtration as a signal on our graph\", y=1.02, fontsize=15)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=rectangle_filtered_dirac)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Rectangle kernel Dirac filtration as a signal on our graph\", y=1.02, fontsize=15)\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the \"diffusion\" of the Diracs induced by the filters. What does it say about the \"communication\" of information across your network? Relate that to the network connectivity measures that you analyzed during the previous milestones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "**Your answer here.**\n",
    "\n",
    "Even if these results are not very significant, we at least observe some kind of diffusion for the three filtered versions of the Diracs. In fact, we can see how the rectangle kernel presents the smoothest diffusion (as we keep a very low t_max=0.1 parameter), followed by the heat kernel and the inverse kernel at last. It is interesting to see how the rectangle kernel only affects nodes represented by the first non-null eigenvector (which correspond to the second lowest frequency).\n",
    "\n",
    "Unfortunately, these eigenmaps don't allow us to analyse such diffusion inside the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - De-noising\n",
    "\n",
    "In this section we will add some centered Gaussian noise to your feature/label signal and attempt to recover it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "In the cell below, set the noise variance $\\sigma^2$ by making sure that the signal-to-noise ratio $SNR = \\frac{\\operatorname{Var}(\\text{labels})}{\\sigma^2}$ is about  $1.5$.\n",
    "\n",
    "_Note:_ Actually, you might want to play with the noise variance here and set it to different values and see how the denoising filters behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "labels_variance = np.var(label_signal)\n",
    "noise_variance = labels_variance / 1.5\n",
    "noisy_measurements = label_signal + noise_variance * np.random.randn(n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "In the denoising setting, a common graph signal processing assumption is that the signal $z$ that we want to recover is \"smooth\", in the sense that $\\|\\nabla_G z\\|_2 = \\sqrt{z^{\\top} L z}$ is small, while remaining \"close\" to the measurements that we start with. This leads to denoising by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "z^\\star = \\text{arg} \\, \\underset{z \\in \\mathbb{R}^{N}}{\\min} \\, \\|z - y\\|_2^2 + \\gamma z^{\\top} L z, \n",
    "$$\n",
    "\n",
    "where $y \\in \\mathbb{R}^{N}$ is the vector of noisy measurements.\n",
    "\n",
    "Derive the close form solution to this problem giving $z^\\star$ as a function of $y$, $\\gamma$ and $L$. Does this solution correspond to any graph filtering operation that you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "$$\\|z - y\\|_2^2 + \\gamma z^{\\top} L z = (z-y)^T(z-y)+\\gamma z^{\\top}Lz$$\n",
    "\n",
    "And when expanding it we get: \n",
    "$$(z^{\\top}z-z^{\\top}y-y^{\\top}z+y^{\\top}y)+\\gamma z^{\\top}Lz$$\n",
    "\n",
    "Now we derive w.r.t z to obtain:\n",
    "$$(2z-2y)^{\\top}+(\\gamma Lz)^{\\top}+\\gamma z^{\\top}L^{\\top}=(z-y)^{\\top}+\\gamma z^{\\top}L^{\\top}=\\textbf{z - y}+\\gamma \\textbf{Lz=0}$$\n",
    "\n",
    "Finally, we isolate z: \n",
    "$$z^\\star = (I+\\gamma L)^{-1}\\cdot y$$\n",
    "\n",
    "It looks like the inverse kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Now, denoise the noisy measurements by passing them through the filters that you implemented in Question 6. Choose the filter parameters based on the behavior of the GFT of your original label signal (this is the prior knowledge that you input to the problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "z_heat_denoised = graph_filter(noisy_measurements, heat_kernel, tau=-1)\n",
    "z_inv_denoised = graph_filter(noisy_measurements, inverse_kernel, tau=1.5)\n",
    "z_rect_denoised = graph_filter(noisy_measurements, rectangle_kernel, l_min=0, l_max=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_err = np.linalg.norm(label_signal - z_rect_denoised) / np.linalg.norm(label_signal)\n",
    "print('The relative error between our label signal and the denoised one is: {err:.2f}'.format(err=rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot, on your graph, the original label signal, the noisy measurements, and the three denoised version obtained above. Report on each plot the value of the corresponding relative error \n",
    "$$\n",
    "\\text{rel-err} = \\frac{\\|\\text{labels} - z \\|_2}{\\|\\text{labels}\\|_2},\n",
    "$$\n",
    "where $z$ is the plotted signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=label_signal)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Original Signal\" , y=1.08, fontsize=15)\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "rel_err = np.linalg.norm(label_signal - noisy_measurements) / np.linalg.norm(label_signal)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=noisy_measurements)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Noisy measurements\\n Relative error: {err:.3f}\".format(err=rel_err), y=1.08, fontsize=15)\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "rel_err = np.linalg.norm(label_signal - z_heat_denoised) / np.linalg.norm(label_signal)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=z_heat_denoised)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Denoised graph signal using Heat Kernel filter\\n Relative error: {err:.3f}\".format(err=rel_err), y=1.08, fontsize=15)\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "rel_err = np.linalg.norm(label_signal - z_inv_denoised) / np.linalg.norm(label_signal)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=z_inv_denoised)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Denoised graph signal using Inverse Kernel filter \\n Relative error: {err:.3f}\".format(err=rel_err), y=1.08, fontsize=15)\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "rel_err = np.linalg.norm(label_signal - z_rect_denoised) / np.linalg.norm(label_signal)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=z_rect_denoised)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Denoised graph signal using Rectangle Kernel filter \\n Relative error: {err:.3f}\".format(err=rel_err), y=1.08, fontsize=15)\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As usual, those visualisation are not very significative.**\n",
    "\n",
    "**Also, we see that there is a problem with the computed relative errors. The error for filtered signals is bigger than the error of the noisy measurements, which wouldn't be the case. This is due to the fact that we are using the normalized Laplacian, and that the way we are asked to compute the relative error is rather adapted to the combinatorial Laplacian.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, overlay on the same plot the GFT of all five signals above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the absolute GFT of all of our signals\n",
    "GFT_label = abs(GFT(label_signal))\n",
    "GFT_noisy = abs(GFT(noisy_measurements))\n",
    "GFT_denoised_heat = abs(GFT(z_heat_denoised))\n",
    "GFT_denoised_inv = abs(GFT(z_inv_denoised))\n",
    "GFT_denoised_rect = abs(GFT(z_rect_denoised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,7))\n",
    "ax.set_title(\"GFT of original, noisy and filtered signals\", y=1.00, fontweight=\"bold\")\n",
    "ax.plot(GFT_label, 'b-', alpha=0.4, label = 'Original')\n",
    "ax.plot(GFT_noisy,'r-', alpha=0.3, label = 'Noise')\n",
    "ax.plot(GFT_denoised_heat,'y-', alpha=0.2, label = 'Heat')\n",
    "ax.plot(GFT_denoised_inv,'c-', alpha=0.2, label = 'Inverse')\n",
    "ax.plot(GFT_denoised_rect,'g-', alpha=0.2, label = 'Rectangle')\n",
    "ax.set_xlabel(\"Eigenvalues\")\n",
    "ax.set_ylabel(\"Magnitude\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,7))\n",
    "ax.set_title(\"Difference (error) between GFTs of denoised signals and original label signal\", y=1.00, fontweight=\"bold\")\n",
    "ax.plot(GFT_label-GFT_denoised_heat,'r-', alpha=0.4, label = 'Heat Filtering')\n",
    "ax.plot(GFT_label-GFT_denoised_inv,'b-', alpha=0.2, label = 'Inverse Filtering')\n",
    "ax.plot(GFT_label-GFT_denoised_rect,'y-', alpha=0.2, label = 'Rectangle Filtering')\n",
    "ax.set_xlabel(\"Eigenvalues\")\n",
    "ax.set_ylabel(\"Magnitude\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Comment on which denoised version seems to best match the original label signal. What is the underlying assumption behind the three filtering approaches? Do you think it holds for your label signal? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "As we can see, heat and inverse kernels give very similar filtering results although the heat kernel is slightly better. Both are much more accurate than the rectangle kernel filtering. However, their performance is not perfect as quite a bit of low frequency noise is still present in the filtered signal.\n",
    "\n",
    "This may happen because our label signal does not respect the underlying assumption for filtering approaches, which is that the signal has to be smooth. In our case, the label signal is a binary signal, not smooth at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-31T13:05:59.301384Z",
     "start_time": "2018-08-31T13:05:59.297336Z"
    }
   },
   "source": [
    "## 4 - Transductive learning\n",
    "\n",
    "It is often the case in large networks that we can only afford to query properties/labels on a small subset of nodes. Nonetheless, if the underlying labels signal is \"regular\" enough, we might still be able to recover a good approximation of it by solving an offline variational problem, with constraints on the values of the measured nodes. \n",
    "\n",
    "In this section, we will be interested in solving such transductive learning problems by minimizing a (semi-) p-norm of the graph gradient applied to the signal of interest:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\, \\underset{z|_S = y}{\\min} \\|\\nabla_G z\\|_p^p,\n",
    "$$\n",
    "\n",
    "where $S$ is the set of measured nodes.\n",
    "\n",
    "In English, we can say that we are looking for solutions with small \"aggregated local variations\", as measured by $\\|\\nabla_G z\\|_p^p = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( \\sqrt{W_{ij}} |z[i] - z[j]| \\right)^p,$ while satisfying the measurement constraints $z[i] = y[i]$ for $i \\in S.$\n",
    "\n",
    "We will work with two cases, according to the choices $p=1$ or $p=2.$ For $p=1,$ the problem is known as \"interpolation by graph total-variation minimization,\" whereas for $p=2$ it is sometimes called \"interpolation by Tikhonov regularization\".\n",
    "\n",
    "In order to solve these variational problems with the black-box solver provided to you, you will use the [pyunlocbox](https://pyunlocbox.readthedocs.io). This toolbox implements iterative solvers based on so-called [\"proximal-splitting\"](https://en.wikipedia.org/wiki/Proximal_gradient_method) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "Throughout this section, we will consider only a binarized version of your label signal. If your variable `labels` currently has values other than $\\{-1, 1\\},$ threshold them so that those are the only values taken in this vector. This can be done for example by choosing a number $t \\in \\mathbb{R}$ and then setting $\\text{labels_bin}[i] = 1$ if $\\text{labels}[i] \\geq t$ and $\\text{labels_bin}[i] = 0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "labels_bin = label_signal.copy()\n",
    "labels_bin[np.where(label_signal == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, subsample this binarized label signal by $70\\%$ by choosing, uniformly at random, $30\\%$ of the nodes whose labels we will keep.\n",
    "\n",
    "You will do this by computing a \"measurement mask\" vector `w` with `1.0`'s at the measured coordinates, and $0.0$'s otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "mn_ratio = 0.3\n",
    "m = int(mn_ratio * n_nodes)  # Number of measurements.\n",
    "\n",
    "w = np.array([0] * (n_nodes - m) + [1] * m)\n",
    "np.random.shuffle(w)\n",
    "w = w.astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the subsampled signal on the graph. _Hint:_ you might want to set to `numpy.nan` the values of the un-measured nodes for a cleaner plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "subsampled_signal = labels_bin[w]\n",
    "\n",
    "subsampled_nodes = list(np.where(w == True))[0]\n",
    "subsampled_graph = graph.subgraph(subsampled_nodes)\n",
    "mapping=dict(zip(subsampled_graph.nodes(),list(range(0, m))))\n",
    "subsampled_graph = nx.relabel_nodes(subsampled_graph, mapping)\n",
    "sub_coords = coords[w, :]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "subnodes = nx.draw_networkx_nodes(subsampled_graph, sub_coords, node_size=60, node_color=subsampled_signal)\n",
    "nx.draw_networkx_edges(subsampled_graph, coords, alpha=0.3)\n",
    "plt.title(\"Subsampled binarized label signal on our graph\")\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(subnodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude\n",
    "\n",
    "For the solution of the variational problems you can use the following function as a \"black-box\". \n",
    "\n",
    "You will just need to provide a `gradient` matrix (which you should already have from Section 0), and an orthogonal projection operator `P` onto the span of the measured coordinates (made precise in the next question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_pnorm_interpolation(gradient, P, x0=None, p=1., **kwargs):\n",
    "    r\"\"\"\n",
    "    Solve an interpolation problem via gradient p-norm minimization.\n",
    "\n",
    "    A signal :math:`x` is estimated from its measurements :math:`y = A(x)` by solving\n",
    "    :math:`\\text{arg}\\underset{z \\in \\mathbb{R}^n}{\\min}\n",
    "    \\| \\nabla_G z \\|_p^p \\text{ subject to } Az = y` \n",
    "    via a primal-dual, forward-backward-forward algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gradient : array_like\n",
    "        A matrix representing the graph gradient operator\n",
    "    P : callable\n",
    "        Orthogonal projection operator mapping points in :math:`z \\in \\mathbb{R}^n` \n",
    "        onto the set satisfying :math:`A P(z) = A z`.\n",
    "    x0 : array_like, optional\n",
    "        Initial point of the iteration. Must be of dimension n.\n",
    "        (Default is `numpy.random.randn(n)`)\n",
    "    p : {1., 2.}\n",
    "    kwargs :\n",
    "        Additional solver parameters, such as maximum number of iterations\n",
    "        (maxit), relative tolerance on the objective (rtol), and verbosity\n",
    "        level (verbosity). See :func:`pyunlocbox.solvers.solve` for the full\n",
    "        list of options.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : array_like\n",
    "        The solution to the optimization problem.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = lambda z: gradient.dot(z)\n",
    "    div = lambda z: gradient.transpose().dot(z)\n",
    "\n",
    "    # Indicator function of the set satisfying :math:`y = A(z)`\n",
    "    f = functions.func()\n",
    "    f._eval = lambda z: 0\n",
    "    f._prox = lambda z, gamma: P(z)\n",
    "\n",
    "    # :math:`\\ell_1` norm of the dual variable :math:`d = \\nabla_G z`\n",
    "    g = functions.func()\n",
    "    g._eval = lambda z: np.sum(np.abs(grad(z)))\n",
    "    g._prox = lambda d, gamma: functions._soft_threshold(d, gamma)\n",
    "\n",
    "    # :math:`\\ell_2` norm of the gradient (for the smooth case)\n",
    "    h = functions.norm_l2(A=grad, At=div)\n",
    "\n",
    "    stepsize = (0.9 / (1. + scipy.sparse.linalg.norm(gradient, ord='fro'))) ** p\n",
    "\n",
    "    solver = solvers.mlfbf(L=grad, Lt=div, step=stepsize)\n",
    "\n",
    "    if p == 1.:\n",
    "        problem = solvers.solve([f, g, functions.dummy()], x0=x0, solver=solver, **kwargs)\n",
    "        return problem['sol']\n",
    "    if p == 2.:\n",
    "        problem = solvers.solve([f, functions.dummy(), h], x0=x0, solver=solver, **kwargs)\n",
    "        return problem['sol']\n",
    "    else:\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "During the iterations of the algorithm used for solving the variational problem, we have to make sure that the labels at the measured nodes stay the same. We will do this by means of an operator `P` which, given a vector $a \\in \\mathbb{R}^{N},$ returns another vector $b \\in \\mathbb{R}^{N}$ satisfying $b[i] = \\text{labels_bin}[i]$ for every node $i$ in the set $S$ of known labels, and $b[i] = a[i]$ otherwise. Write in the cell below the function for this orthogonal projection operator `P`.\n",
    "\n",
    "_Hint:_ remember you have already computed the mask `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def P(a):\n",
    "    b = a.copy()\n",
    "    # The set of known labels is the set of subsampled labels that is described by the mask w\n",
    "    b[w] = labels_bin[w]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "### Question 15\n",
    "\n",
    "Solve the variational problems for $p = 1$ and $p = 2$. Record the solution for the $1-$norm minimization under `sol_1norm_min` and the one for $2-$norm minimization under `sol_2norm_min`.\n",
    "\n",
    "Compute also binarized versions of these solutions by thresholding the values with respect to $0$, that is, non-negative values become `1.0`, while negative values become `-1.0`. Store those binarized versions under `sol_1norm_bin` and `sol_2norm_bin`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since x0 was not initialized we initialized it to random.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = np.random.randn(n_nodes)\n",
    "\n",
    "sol_1norm_min = graph_pnorm_interpolation(gradient, P, x0=x0, p=1)\n",
    "\n",
    "sol_2norm_min = graph_pnorm_interpolation(gradient, P, x0=x0, p=2)\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "sol_1norm_bin = [1 if i>threshold else -1 for i in sol_1norm_min]\n",
    "\n",
    "sol_2norm_bin = [1 if i>threshold else -1 for i in sol_2norm_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Plot, on your graph, the original `labels_bin` signal, as well as the solutions to the variational problems (both binarized and otherwise). Indicate on each plot the value of the relative error $\\text{rel-err} = \\frac{\\|\\text{labels_bin} - z\\|_2}{\\|\\text{labels_bin}\\|_2}$, where $z$ is the signal in the corresponding plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=labels_bin)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Binary label signal\")\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "rel_err = np.linalg.norm(labels_bin - sol_1norm_min) / np.linalg.norm(labels_bin)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=sol_1norm_min)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Solution to the variational problem with p=1 norm \\n Relative error: {err:.3f}\".format(err=rel_err))\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "rel_err = np.linalg.norm(labels_bin - sol_2norm_min) / np.linalg.norm(labels_bin)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=sol_2norm_min)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Solution to the variational problem with p=2 norm \\n Relative error: {err:.3f}\".format(err=rel_err))\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "rel_err = np.linalg.norm(labels_bin - sol_1norm_bin) / np.linalg.norm(labels_bin)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=sol_1norm_bin)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Binarized solution to the variational problem with p=1 norm \\n Relative error: {err:.3f}\".format(err=rel_err))\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "rel_err = np.linalg.norm(labels_bin - sol_2norm_bin) / np.linalg.norm(labels_bin)\n",
    "nodes = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=sol_2norm_bin)\n",
    "nx.draw_networkx_edges(graph, coords, alpha=0.3)\n",
    "plt.title(\"Binarized solution to the variational problem with p=2 norm \\n Relative error: {err:.3f}\".format(err=rel_err))\n",
    "plt.xlabel(\"Second eigenvector\")\n",
    "plt.ylabel(\"Fourth eigenvector\")\n",
    "plt.colorbar(nodes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "\n",
    "Now that you have got a feeling for the sort of solutions that the transductive learning problems studied can give, we will see what is the effect of the number of measurements on the accuracy of both $p-$norm minimization problems.\n",
    "\n",
    "Towards this goal, you will write a `phase_transition()` function. This function will basically go over all the procedures that you have implemented in this section, but for varying numbers of measurements and thresholding values. It will also compute the relative error, $\\text{rel-err},$ of the solutions and average them over a number of trials.\n",
    "\n",
    "The output of the `phase_transition()` function has to be a matrix with `len(mn_ratios)` columns and `len(thresholds)` rows. Each pixel $(i,j)$ in the output matrix has to contain the average, over `n_trials` trials, of the relative error $\\text{rel-err}$ in the binarized (with threshold `thresholds[i]`) solution given by `graph_pnorm_interpolation()` from observing an `mn_ratios[j]` fraction of nodes. The randomness comes from a different choice of mask `w` at each trial, hence the averaging.\n",
    "\n",
    "The interest of this phase transition matrix is to assess what level of recovery error one could expect for a certain fraction of measurements and a certain threshold level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def phase_transition(mn_ratios, thresholds, n_trials, labels_bin, p):\n",
    "\n",
    "    pt_matrix = np.zeros((len(thresholds), len(mn_ratios)))\n",
    "    for (i, thresh) in enumerate(thresholds):\n",
    "        for (j, ratio) in enumerate(mn_ratios):\n",
    "            err = np.zeros((n_trials, 1))\n",
    "            for t in range(n_trials):\n",
    "                # Create sample mask.\n",
    "                m = int(ratio * n_nodes)  # Number of measurements.\n",
    "\n",
    "                w = np.array([0] * (n_nodes - m) + [1] * m)\n",
    "                np.random.shuffle(w)\n",
    "                w = w.astype(bool)\n",
    "\n",
    "                # Solve p-norm interpolation.\n",
    "                x0 = np.random.randn(n_nodes)\n",
    "                sol = graph_pnorm_interpolation(gradient, P, x0=x0, p=p)\n",
    "                err[t] = np.linalg.norm(labels_bin - sol) / np.linalg.norm(labels_bin)\n",
    "\n",
    "            # Aggregate.\n",
    "            pt_matrix[i,j] = np.mean(err)\n",
    "    \n",
    "    return pt_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17\n",
    "\n",
    "Pick 5 \"m/n\" ratios in $(0, 1)$ and 5 threshold levels in $(-1, 1)$ and run the `phase_transition()` function with `n_trials` = 20, for both $p = 1$ and $p = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously saw that our values were around 0 most of the time. This is why our thresholds are focused around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "mn_ratios = [0.15, 0.3, 0.45, 0.6, 0.75]\n",
    "\n",
    "thresholds = [-0.4, -0.2, 0, 0.2, 0.4]\n",
    "\n",
    "pt_matrix_1norm = phase_transition(mn_ratios, thresholds, 20, labels_bin, p=1)\n",
    "\n",
    "pt_matrix_2norm = phase_transition(mn_ratios, thresholds, 20, labels_bin, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both phase transition matrices as images with a colorbar. Make sure to properly name the axes and title the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "names_x = ['Ratio: '+str(0.15), 'Ratio: '+str(0.3), 'Ratio: '+str(0.45), 'Ratio: '+str(0.6), 'Ratio: '+str(0.75)]\n",
    "names_y = ['Thresh: '+str(-0.9), 'Thresh: '+str(-0.45), 'Thresh: '+str(0), 'Thresh: '+str(0.45), 'Thresh: '+str(0.9)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 5.1))\n",
    "fig.suptitle(\"Phase transition matrices\",fontweight=\"bold\")\n",
    "\n",
    "min_data = min(np.min(pt_matrix_1norm), np.min(pt_matrix_2norm))\n",
    "max_data = max(np.max(pt_matrix_1norm), np.max(pt_matrix_2norm))\n",
    "\n",
    "im1 = ax1.imshow(pt_matrix_1norm, vmin=min_data, vmax=max_data, cmap='Reds')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax1.set_xticks(np.arange(len(names_x)))\n",
    "ax1.set_yticks(np.arange(len(names_y)))\n",
    "# ... and label them with the respective list entries\n",
    "ax1.set_xticklabels(names_x)\n",
    "ax1.set_yticklabels(names_y)\n",
    "ax1.set_title('Norm p = 1')\n",
    "\n",
    "im2 = ax2.imshow(pt_matrix_2norm, vmin=min_data, vmax=max_data, cmap='Reds')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax2.set_xticks(np.arange(len(names_x)))\n",
    "ax2.set_yticks(np.arange(len(names_y)))\n",
    "# ... and label them with the respective list entries\n",
    "ax2.set_xticklabels(names_x)\n",
    "ax2.set_yticklabels(names_y)\n",
    "ax2.set_title('Norm p = 2')\n",
    "\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "fig.colorbar(im2, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "min_data = np.min(pt_matrix_2norm)\n",
    "max_data = np.max(pt_matrix_2norm)\n",
    "\n",
    "im = ax.imshow(pt_matrix_2norm, vmin=min_data, vmax=max_data, cmap='Reds')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(names_x)))\n",
    "ax.set_yticks(np.arange(len(names_y)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(names_x)\n",
    "ax.set_yticklabels(names_y)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(names_y)):\n",
    "    for j in range(len(names_x)):\n",
    "        string = \"{0:.2f}\".format(pt_matrix_2norm[i, j])\n",
    "        text = ax.text(j, i, string,\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "ax.set_title('Norm p = 2 with adapted scale', fontweight='bold')\n",
    "fig.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18\n",
    "\n",
    "Do the phase transition plots above provide any justification for choosing one $p-$norm interpolation over the other? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "We purposely plotted both matrices using the same scale. As we can see, the p=1 matrix systematically has higher relative errors than the p=2 matrix. The p=2 matrix has the lowest error of both scales combined throughout the whole matrix. However, the relative error of the p=2 matrix is only $0.3$ lower than the highest error in the p=1 matrix, so we might nuance our claim with the fact that $0.3$ might not be significant (even though it means it is $25\\%$ lower).\n",
    "\n",
    "Moreover, when using the p=2 norm, we can observe that the relative error doesn't change much when the parameters (i.e. threshold and subsampling ratio) change, which indicates a more stable approach. \n",
    "\n",
    "We can conclude that the method that yields the least error between the predicted and the actual labels is using the p=2 norm in the optimization problem.\n",
    "\n",
    "The best parameters when using this norm seem to be: \n",
    "- an extreme threshold ($\\pm 0.9$) and a low subsampling ratio ($0.15$ / $0.3$) \n",
    "- a high ratio ($0.75$) and a medium threshold ($-0.45$ / $0$)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nteract": {
   "version": "0.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
